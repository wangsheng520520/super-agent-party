import base64
import json
import socket
import ipaddress
import aiohttp
import os
from urllib.parse import urlparse, urlunparse, urljoin
from urllib.robotparser import RobotFileParser
from py.get_setting import load_settings, get_host, get_port # 确保导入了这两个函数
from openai import AsyncOpenAI
from ollama import AsyncClient as OllamaClient

# ================= 安全配置 =================

# 建议包含项目地址，方便站长识别
USER_AGENT = "Mozilla/5.0 (compatible; OpenSourceImageBot/1.0)"
ROBOTS_CACHE = {}

def is_private_ip(hostname):
    """检测是否为私有/内网IP，放行代理软件的 Fake-IP (198.18.0.0/15)"""
    if not hostname:
        return False
    try:
        addr_info = socket.getaddrinfo(hostname, None, proto=socket.IPPROTO_TCP)
        fake_ip_net = ipaddress.ip_network('198.18.0.0/15')
        for item in addr_info:
            ip_str = item[4][0]
            ip_obj = ipaddress.ip_address(ip_str)
            # 放行代理软件网段
            if ip_obj in fake_ip_net:
                return False 
            # 拦截真实的内网/本地回环地址
            if ip_obj.is_private or ip_obj.is_loopback:
                print(f"[安全拦截] 图像请求解析到了内网IP: {ip_str}")
                return True
    except:
        return False
    return False

async def check_robots_txt(url):
    """异步检查图像抓取是否符合 robots.txt"""
    parsed = urlparse(url)
    base_url = f"{parsed.scheme}://{parsed.netloc}"
    if base_url in ROBOTS_CACHE:
        return ROBOTS_CACHE[base_url].can_fetch(USER_AGENT, url)
    
    robots_url = urljoin(base_url, "/robots.txt")
    rp = RobotFileParser()
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=3)) as session:
            async with session.get(robots_url) as resp:
                if resp.status == 200:
                    text_data = await resp.text()
                    rp.parse(text_data.splitlines())
                else:
                    rp.allow_all = True
    except:
        rp.allow_all = True
    ROBOTS_CACHE[base_url] = rp
    return rp.can_fetch(USER_AGENT, url)

# ================= 核心功能修改 =================

async def get_image_base64(image_url: str) -> str:
    """
    下载图片并转换为base64编码
    - 对内：仅允许 'uploaded_files' 路径并重定向至本地
    - 对外：遵守 robots.txt 并拦截内网 IP (放行 Fake-IP)
    """
    parsed_url = urlparse(image_url)
    target_url = image_url
    
    # --- 场景 1: 内部文件路径处理 ---
    if 'uploaded_files' in parsed_url.path:
        HOST = get_host()
        PORT = get_port()
        if HOST == '0.0.0.0': HOST = '127.0.0.1'
        # 强制转换为本地安全地址
        modified_parsed = parsed_url._replace(netloc=f'{HOST}:{PORT}')
        target_url = urlunparse(modified_parsed)
        # 内部请求不需要检查 robots 和 IP 安全
    
    # --- 场景 2: 公网 URL 爬取 ---
    else:
        # A. SSRF 安全拦截
        if is_private_ip(parsed_url.hostname):
            raise PermissionError(f"安全拒绝: 不允许从内部网络获取图像 ({parsed_url.hostname})")
        
        # B. Robots.txt 检查
        if not await check_robots_txt(image_url):
            raise PermissionError(f"合规拒绝: 目标网站禁止爬虫抓取该图像")

    # --- 执行下载 ---
    async with aiohttp.ClientSession() as session:
        headers = {'User-Agent': USER_AGENT}
        async with session.get(target_url, headers=headers, timeout=20) as response:
            if response.status != 200:
                raise ValueError(f"无法下载图片: HTTP {response.status} 来自 {image_url}")
            
            # 可选：增加文件大小限制，防止 OOM (内存溢出)
            # if int(response.headers.get('Content-Length', 0)) > 10 * 1024 * 1024:
            #     raise ValueError("图片太大 (超过 10MB)")
                
            image_data = await response.read()
            return base64.b64encode(image_data).decode('utf-8')


async def get_llm_tool(settings):
    llm_list = []

    llmTools = settings['llmTools']

    for llmTool in llmTools:
        if llmTool['enabled']:
            llm_list.append({"name": llmTool['name'], "description": llmTool['description']})
    if len(llm_list) > 0:
        llm_list = json.dumps(llm_list, ensure_ascii=False, indent=4)
        llm_tool = {
            "type": "function",
            "function": {
                "name": "custom_llm_tool",
                "description": f"custom_llm_tool工具可以调用工具列表中的通用工具。请不要混淆custom_llm_tool和tool_name字段要填入的工具名称。以下是工具列表：\n{llm_list}\n\n如果LLM工具返回的内容包含图片，则返回的图片URL或本地路径，请直接写成：![image](图片URL)格式发给用户，用户就能看到图片了",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "tool_name": {
                            "type": "string",
                            "description": "需要调用的工具名称",
                        },
                        "query": {
                            "type": "string",
                            "description": "需要向工具发送的问题",
                        },
                        "image_url": {
                            "type": "string",
                            "description": "需要向工具发送的图片URL，可选，来自本地服务器上的图片URL也可以填入（例如：http://127.0.0.1:3456/xxx.jpg），会被自动处理为base64编码发送",
                        }
                    },
                    "required": ["tool_name","query"]
                }
            }
        }
        return llm_tool
    else:
        return None
        
async def get_image_media_type(image_url: str) -> str:
    # 根据image_url类型调整
    if image_url.endswith('.png'):
        media_type = 'image/png'
    elif image_url.endswith('.jpg') or image_url.endswith('.jpeg'):
        media_type = 'image/jpeg'
    elif image_url.endswith('.webp'):
        media_type = 'image/webp'
    elif image_url.endswith('.gif'):
        media_type = 'image/gif'
    elif image_url.endswith('.bmp'):
        media_type = 'image/bmp'
    elif image_url.endswith('.tiff'):
        media_type = 'image/tiff'
    elif image_url.endswith('.ico'):
        media_type = 'image/x-icon'
    elif image_url.endswith('.svg'):
        media_type = 'image/svg+xml'
    else:
        media_type = 'image/png'
    return media_type

async def custom_llm_tool(tool_name, query, image_url=None):
    print(f"调用LLM工具：{tool_name}")
    settings = await load_settings()
    llmTools = settings['llmTools']
    for llmTool in llmTools:
        if llmTool['enabled'] and llmTool['name'] == tool_name:
            if llmTool['type'] == 'ollama':
                client = OllamaClient(host=llmTool['base_url'])
                try:
                    content = query
                    
                    # 处理图片输入
                    if image_url:
                        base64_image = await get_image_base64(image_url)
                        media_type = await get_image_media_type(image_url)
                        content = [
                            {"type": "text", "text": query},
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": media_type,
                                    "data": base64_image
                                }
                            }
                        ]

                    # 调用Ollama API
                    response = await client.chat(
                        model=llmTool['model'],
                        messages=[{"role": "user", "content": content}],
                    )
                    return response.message.content
                except Exception as e:
                    return str(e)
            else:
                client = AsyncOpenAI(api_key=llmTool['api_key'],base_url=llmTool['base_url'])
                try:
                    if image_url:
                        base64_image = await get_image_base64(image_url)
                        # 根据image_url类型调整
                        media_type = await get_image_media_type(image_url)
                        prompt = [
                            {
                                "type": "image",
                                "image_url": {"url": f"data:{media_type};base64,{base64_image}"},
                            },
                            {
                                "type": "text",
                                "text": query
                            }
                        ]
                        response = await client.chat.completions.create(
                            model=llmTool['model'],
                            messages=[
                                {"role": "user", "content": prompt},
                            ],
                        )
                    else:
                        response = await client.chat.completions.create(
                            model=llmTool['model'],
                            messages=[
                                {"role": "user", "content": query},
                            ],
                        )
                    return response.choices[0].message.content
                except Exception as e:
                    return str(e)